library(SnowballC) # Word stemming library
# Stemming example
wordStem(c("learn", "learned", "learning", "learns"))
# Use stemDocument() method to apply stemming to entire corpus
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# Remove white spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(tolower = TRUE,removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
sms_dtm
sms_dtm2  #It uses slightly a different stopwords method, so result may vary
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
library(tm)
df <- read.csv("C://Users//Anmol//Desktop//Classification//sms_spam.csv",stringsAsFactors = FALSE)
#anlaysing the data
head(df)
str(df)
summary(df)
table(df$type)
sms_corpus <- VCorpus(VectorSource(df$text))
#inspecting the corporus #list indexing
sms_corpus[[1]]
sms_corpus[[2]]
#viewing the metadata
meta(sms_corpus[[1]])
meta(sms_corpus[[2]])
meta(sms_corpus[[3]])
#viewing the characters
as.character(sms_corpus[[1]])
lapply(sms_corpus[[1:2]], as.character)
sms_corpus_clean <- tm_map(sms_corpus,content_transformer(tolower))
sms_corpus_clean <- tm_map(sms_corpus_clean,removeNumbers)
#removing punctuations
sms_corpus_clean <- tm_map(sms_corpus_clean,removePunctuation)
#removing the stop words
sms_corpus_clean <- tm_map(sms_corpus_clean,removeWords,stopwords())
library(SnowballC)
#testing the function
wordStem(c("learning","learn","learned","learner"))
sms_corpus_clean <- tm_map(sms_corpus_clean,stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean,stripWhitespace)
#creating a word matrix
sms_dttm <- DocumentTermMatrix(sms_corpus_clean)
#creating the sample
str(sms_dttm)
sms_train <- sms_dttm[1:4169,]
sms_test <- sms_dttm[4170:5599,]
#creating the sample
str(sms_dttm)
summary(sms_dttm)
summary(sms_dttm)
#creating the sample
str(sms_dttm)
library(tm)
df <- read.csv("C://Users//Anmol//Desktop//Classification//sms_spam.csv",stringsAsFactors = FALSE)
#anlaysing the data
head(df)
str(df)
summary(df)
table(df$type)
sms_corpus <- VCorpus(VectorSource(df$text))
#inspecting the corporus #list indexing
sms_corpus[[1]]
sms_corpus[[2]]
#viewing the metadata
meta(sms_corpus[[1]])
meta(sms_corpus[[2]])
meta(sms_corpus[[3]])
#viewing the characters
as.character(sms_corpus[[1]])
lapply(sms_corpus[[1:2]], as.character)
sms_corpus_clean <- tm_map(sms_corpus,content_transformer(tolower))
sms_corpus_clean <- tm_map(sms_corpus_clean,removeNumbers)
#removing the stop words
sms_corpus_clean <- tm_map(sms_corpus_clean,removeWords,stopwords())
#removing punctuations
sms_corpus_clean <- tm_map(sms_corpus_clean,removePunctuation)
library(SnowballC)
#testing the function
wordStem(c("learning","learn","learned","learner"))
sms_corpus_clean <- tm_map(sms_corpus_clean,stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean,stripWhitespace)
#creating a word matrix
sms_dttm <- DocumentTermMatrix(sms_corpus_clean)
#creating the sample
str(sms_dttm)
sms_train <-sms_dttm[1:4169]
sms_test <- sms_dttm[4170:5574]
str(sms_train)
#adding labels
sms_test_label <- df[1:4169]$type
#adding labels
sms_test_label <- df[1:4169,]$type
sms_train_lable <- df[4170:5574,]$type
table(sms_test_label)
table(sms_train_lable)
#adding labels
sms_train_label <- df[1:4169,]$type
sms_test_lable <- df[4170:5574,]$type
table(sms_test_label)
table(sms_train_lable)
#visualising the trained dataset
library(wordcloud)
wordcloud(sms_corpus_clean,min.freq = 50,random.order = FALSE)
spam <- subset(df,type=="Spam")
ham <- subset(df,type=="Ham")
wordcloud(spam,min.freq = 50,random.order = FALSE)
wordcloud(spam$text,min.freq = 50,random.order = FALSE)
wordcloud(spam$text,max.words = 100,scale = c(3,.05))
# real and non-enconded messages
# tagged as legitimate (ham) or spam.
#
# 747 SPAM + 4827 HAM = 5574 SMSs
#
# Examples:
#
# ham   What you doing?how are you?
# spam  URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize
#===================================================
library(tm) # Text mining package
#---------------------------------------------------
# 1. Explore and prepare the data
#---------------------------------------------------
sms_raw <- read.csv(file.choose(), stringsAsFactors = FALSE)
# see that the sms_raw data frame includes 5,559 total
# SMS messages with two features: type and text.
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
# How many are HAM and how many are SPAM?
table(sms_raw$type)
# vectorSource - It treats each elemnt in the vector as a document
# i.e. each SMS as a document
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
# Inspect the first and second SMS messages in the corpus:
inspect(sms_corpus[1:2])
# Print the metadata of 1 document = 1 SMS
meta(sms_corpus[[1]])
meta(sms_corpus[[2]])
meta(sms_corpus[[3]])
# How to view the original SMS?? View 1st SMS.
as.character(sms_corpus[[1]])
# How to view multiple SMSs? Use lapply().
lapply(sms_corpus[1:2], as.character)
# standardize the messages to use only lowercase characters.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# Check whether tolower() worked or not.
as.character(sms_corpus[[1]])
# Remove numbers from SMSs, bcoz they wont give any useful information
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
# Remove stop words - a, and, an, the etc. (There are 571 words in tm package)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
# eliminate any punctuation from the text messages
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
library(SnowballC) # Word stemming library
# Stemming example
wordStem(c("learn", "learned", "learning", "learns"))
# Use stemDocument() method to apply stemming to entire corpus
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# Remove white spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(tolower = TRUE,removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
sms_dtm
sms_dtm2  #It uses slightly a different stopwords method, so result may vary
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
library(wordcloud)
# random.order = FALSE, the cloud will be arranged in a nonrandom order with higher
# frequency words placed closer to the center.
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 100, scale = c(3, 0.5))
spam <- subset(df,type=="spam")
ham <- subset(df,type=="ham")
wordcloud(spam$text,max.words = 100,scale = c(3,.05))
wordcloud(spam$text,max.words = 100,scale = c(3,0.5))
wordcloud(ham$text,max.words = 100,scale = c(3,0.5))
library(tm)
#input of file
df <- read.csv("C://Users//Anmol//Desktop//Classification//sms_spam.csv",stringsAsFactors = FALSE)
#anlaysing the data
head(df)
str(df)
summary(df)
#getting count of ham and spam
table(df$type)
#creating corpus
sms_corpus <- VCorpus(VectorSource(df$text))
#inspecting the corporus #list indexing
sms_corpus[[1]]
sms_corpus[[2]]
#viewing the metadata
meta(sms_corpus[[1]])
meta(sms_corpus[[2]])
meta(sms_corpus[[3]])
#viewing the characters
as.character(sms_corpus[[1]])
lapply(sms_corpus[[1:2]], as.character)
#cleaning the data #upper to lower
sms_corpus_clean <- tm_map(sms_corpus,content_transformer(tolower))
#removing numbers from the messages
sms_corpus_clean <- tm_map(sms_corpus_clean,removeNumbers)
#removing the stop words
sms_corpus_clean <- tm_map(sms_corpus_clean,removeWords,stopwords())
#removing punctuations
sms_corpus_clean <- tm_map(sms_corpus_clean,removePunctuation)
#steeming for removing the text with similar base class
library(SnowballC)
#testing the function
wordStem(c("learning","learn","learned","learner"))
sms_corpus_clean <- tm_map(sms_corpus_clean,stemDocument)
#stripping the whitespaces
sms_corpus_clean <- tm_map(sms_corpus_clean,stripWhitespace)
#creating a word matrix
sms_dttm <- DocumentTermMatrix(sms_corpus_clean)
#creating the sample
str(sms_dttm)
sms_train <-sms_dttm[1:4169]
sms_test <- sms_dttm[4170:5574]
#adding labels
sms_train_label <- df[1:4169,]$type
sms_test_lable <- df[4170:5574,]$type
#seeing the ditribution
table(sms_test_label)
table(sms_train_lable)
#visualising the trained dataset
library(wordcloud)
wordcloud(sms_corpus_clean,min.freq = 50,random.order = FALSE)
#comparing cloud of spam and ham
spam <- subset(df,type=="spam")
ham <- subset(df,type=="ham")
wordcloud(spam$text,max.words = 100,scale = c(3,0.5))
wordcloud(ham$text,max.words = 100,scale = c(3,0.5))
#creating frequent words
findFreqTerms()
#creating frequent words
findFreqTerms(sms_train_label,5)
#creating frequent words
findFreqTerms(sms_train,5)
sms_train <-sms_dttm[1:4169]
sms_test <- sms_dttm[4170:5574]
#adding labels
sms_train_label <- df[1:4169,]$type
sms_test_lable <- df[4170:5574,]$type
#seeing the ditribution
table(sms_test_label)
#adding labels
sms_train_label <- df[1:4169,]$type
sms_test_lable <- df[4170:5574,]$type
#seeing the ditribution
table(sms_test_label)
#seeing the ditribution
table(sms_test_label)
table(sms_train_lable)
df <- read.csv("C://Users//Anmol//Desktop//Classification//sms_spam.csv",stringsAsFactors = FALSE)
library(tm)
df <- read.csv("C://Users//Anmol//Desktop//Classification//sms_spam.csv",stringsAsFactors = FALSE)
#anlaysing the data
head(df)
str(df)
summary(df)
table(df$type)
sms_corpus <- VCorpus(VectorSource(df$text))
#inspecting the corporus #list indexing
sms_corpus[[1]]
sms_corpus[[2]]
#viewing the metadata
meta(sms_corpus[[1]])
meta(sms_corpus[[2]])
meta(sms_corpus[[3]])
#viewing the characters
as.character(sms_corpus[[1]])
lapply(sms_corpus[[1:2]], as.character)
sms_corpus_clean <- tm_map(sms_corpus,content_transformer(tolower))
sms_corpus_clean <- tm_map(sms_corpus_clean,removeNumbers)
#removing the stop words
sms_corpus_clean <- tm_map(sms_corpus_clean,removeWords,stopwords())
#removing punctuations
sms_corpus_clean <- tm_map(sms_corpus_clean,removePunctuation)
library(SnowballC)
#testing the function
wordStem(c("learning","learn","learned","learner"))
sms_corpus_clean <- tm_map(sms_corpus_clean,stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean,stripWhitespace)
#creating a word matrix
sms_dttm <- DocumentTermMatrix(sms_corpus_clean)
#creating the sample
str(sms_dttm)
sms_train <-sms_dttm[1:4169]
sms_test <- sms_dttm[4170:5574]
#adding labels
sms_train_label <- df[1:4169,]$type
sms_test_lable <- df[4170:5574,]$type
#seeing the ditribution
table(sms_test_label)
sms_train <-sms_dttm[1:4169]
#creating the sample
str(sms_dttm)
sms_train <-sms_dttm[1:4169]
sms_test <- sms_dttm[4170:5574]
#adding labels
sms_train_label <- df[1:4169,]$type
sms_test_lable <- df[4170:5574,]$type
#seeing the ditribution
table(sms_test_label)
table(sms_train_lable)
#visualising the trained dataset
library(wordcloud)
wordcloud(sms_corpus_clean,min.freq = 50,random.order = FALSE)
spam <- subset(df,type=="spam")
ham <- subset(df,type=="ham")
spam <- subset(df,type=="spam")
ham <- subset(df,type=="ham")
wordcloud(spam$text,max.words = 100,scale = c(3,0.5))
wordcloud(ham$text,max.words = 100,scale = c(3,0.5))
#creating frequent words
findFreqTerms(sms_train,5)
# real and non-enconded messages
# tagged as legitimate (ham) or spam.
#
# 747 SPAM + 4827 HAM = 5574 SMSs
#
# Examples:
#
# ham   What you doing?how are you?
# spam  URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize
#===================================================
library(tm) # Text mining package
#---------------------------------------------------
# 1. Explore and prepare the data
#---------------------------------------------------
sms_raw <- read.csv(file.choose(), stringsAsFactors = FALSE)
# see that the sms_raw data frame includes 5,559 total
# SMS messages with two features: type and text.
str(sms_raw)
sms_raw$type <- factor(sms_raw$type)
str(sms_raw$type)
# How many are HAM and how many are SPAM?
table(sms_raw$type)
# vectorSource - It treats each elemnt in the vector as a document
# i.e. each SMS as a document
sms_corpus <- VCorpus(VectorSource(sms_raw$text))
print(sms_corpus)
# Inspect the first and second SMS messages in the corpus:
inspect(sms_corpus[1:2])
# Print the metadata of 1 document = 1 SMS
meta(sms_corpus[[1]])
meta(sms_corpus[[2]])
meta(sms_corpus[[3]])
# How to view the original SMS?? View 1st SMS.
as.character(sms_corpus[[1]])
# How to view multiple SMSs? Use lapply().
lapply(sms_corpus[1:2], as.character)
# standardize the messages to use only lowercase characters.
sms_corpus_clean <- tm_map(sms_corpus, content_transformer(tolower))
# Check whether tolower() worked or not.
as.character(sms_corpus[[1]])
# Remove numbers from SMSs, bcoz they wont give any useful information
sms_corpus_clean <- tm_map(sms_corpus_clean, removeNumbers)
# Remove stop words - a, and, an, the etc. (There are 571 words in tm package)
sms_corpus_clean <- tm_map(sms_corpus_clean, removeWords, stopwords())
library(SnowballC) # Word stemming library
# eliminate any punctuation from the text messages
sms_corpus_clean <- tm_map(sms_corpus_clean, removePunctuation)
# Stemming example
wordStem(c("learn", "learned", "learning", "learns"))
# Use stemDocument() method to apply stemming to entire corpus
sms_corpus_clean <- tm_map(sms_corpus_clean, stemDocument)
# Remove white spaces
sms_corpus_clean <- tm_map(sms_corpus_clean, stripWhitespace)
sms_dtm <- DocumentTermMatrix(sms_corpus_clean)
sms_dtm2 <- DocumentTermMatrix(sms_corpus, control = list(tolower = TRUE,removeNumbers = TRUE,
stopwords = TRUE,
removePunctuation = TRUE,
stemming = TRUE
))
sms_dtm
sms_dtm2  #It uses slightly a different stopwords method, so result may vary
sms_dtm_train <- sms_dtm[1:4169, ]
sms_dtm_test <- sms_dtm[4170:5559, ]
sms_train_labels <- sms_raw[1:4169, ]$type
sms_test_labels <- sms_raw[4170:5559, ]$type
prop.table(table(sms_train_labels))
prop.table(table(sms_test_labels))
library(wordcloud)
# random.order = FALSE, the cloud will be arranged in a nonrandom order with higher
# frequency words placed closer to the center.
wordcloud(sms_corpus_clean, min.freq = 50, random.order = FALSE)
spam <- subset(sms_raw, type == "spam")
ham <- subset(sms_raw, type == "ham")
wordcloud(spam$text, max.words = 100, scale = c(3, 0.5))
wordcloud(ham$text, max.words = 100, scale = c(3, 0.5))
#  the words appearing at least five times in the sms_dtm_train matrix
findFreqTerms(sms_dtm_train, 5)
sms_freq_words <- findFreqTerms(sms_dtm_train, 5)
print(sms_freq_words)
# Get only the words which are present in sms_freq_words
# The training and test datasets now include 1,136 features, which correspond to
# words appearing in at least five messages.
sms_dtm_freq_train<- sms_dtm_train[ , sms_freq_words]
sms_dtm_freq_test <- sms_dtm_test[ , sms_freq_words]
print(sms_dtm_freq_test)
convert_counts <- function(x) {
x <- ifelse(x > 0, "Yes", "No")
}
# MARGIN = 2, apply on column
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts)
print(sms_train[1:5,])
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
# MARGIN = 2, apply on column
sms_train <- apply(sms_dtm_freq_train, MARGIN = 2,
convert_counts)
print(sms_train[1:5,])
sms_test <- apply(sms_dtm_freq_test, MARGIN = 2,
convert_counts)
library(e1071)
sms_classifier <- naiveBayes(sms_train, sms_train_labels)
sms_test_pred <- predict(sms_classifier, sms_test)
library(gmodels) # Various R programming tools for model fitting.
CrossTable(sms_test_pred, sms_test_labels,
dnn = c('predicted', 'actual'))
sms_classifier2 <- naiveBayes(sms_train, sms_train_labels,laplace = 2)
sms_test_pred2 <- predict(sms_classifier2, sms_test)
CrossTable(sms_test_pred2, sms_test_labels,
prop.chisq = FALSE, prop.t = FALSE, prop.r = FALSE,
dnn = c('predicted', 'actual'))
df$type <- factor(df$type)
library(tm)
df <- read.csv("C://Users//Anmol//Desktop//Classification//sms_spam.csv",stringsAsFactors = FALSE)
#anlaysing the data
head(df)
str(df)
summary(df)
df$type <- factor(df$type)
table(df$type)
sms_corpus <- VCorpus(VectorSource(df$text))
#inspecting the corporus #list indexing
sms_corpus[[1]]
sms_corpus[[2]]
#viewing the metadata
meta(sms_corpus[[1]])
meta(sms_corpus[[2]])
meta(sms_corpus[[3]])
#viewing the characters
as.character(sms_corpus[[1]])
lapply(sms_corpus[[1:2]], as.character)
sms_corpus_clean <- tm_map(sms_corpus,content_transformer(tolower))
sms_corpus_clean <- tm_map(sms_corpus_clean,removeNumbers)
#removing the stop words
sms_corpus_clean <- tm_map(sms_corpus_clean,removeWords,stopwords())
#removing punctuations
sms_corpus_clean <- tm_map(sms_corpus_clean,removePunctuation)
library(SnowballC)
#testing the function
wordStem(c("learning","learn","learned","learner"))
sms_corpus_clean <- tm_map(sms_corpus_clean,stemDocument)
sms_corpus_clean <- tm_map(sms_corpus_clean,stripWhitespace)
#creating a word matrix
sms_dttm <- DocumentTermMatrix(sms_corpus_clean)
#creating the sample
str(sms_dttm)
sms_train <-sms_dttm[1:4169]
sms_test <- sms_dttm[4170:5574]
#adding labels
sms_train_label <- df[1:4169,]$type
sms_test_lable <- df[4170:5574,]$type
#seeing the ditribution
table(sms_test_label)
sms_train <-sms_dttm[1:4169]
sms_test <- sms_dttm[4170:5574]
#adding labels
sms_train_label <- df[1:4169,]$type
sms_test_lable <- df[4170:5574,]$type
sms_test <-sms_dttm[1:4169]
sms_train <-sms_dttm[1:4169]
sms_test <- sms_dttm[4170:5574]
#adding labels
sms_train_label <- df[1:4169,]$type
sms_test_lable <- df[4170:5574,]$type
#seeing the ditribution
table(sms_test_label)
table(sms_train_lable)
#visualising the trained dataset
library(wordcloud)
spam <- subset(df,type=="spam")
ham <- subset(df,type=="ham")
wordcloud(spam$text,max.words = 100,scale = c(3,0.5))
wordcloud(ham$text,max.words = 100,scale = c(3,0.5))
#creating frequent words
findFreqTerms(sms_train,5)
#creating frequent words
findFreqTerms(sms_train,5)
